{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Project\n",
    "\n",
    "## Data Description\n",
    "Recognizing multi-digit numbers in photographs captured at street level is an important component of modernday map making. A classic example of a corpus of such street-level photographs is Googleâ€™s Street View\n",
    "imagery comprised of hundreds of millions of geo-located 360-degree panoramic images. The ability to\n",
    "automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed\n",
    "number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building\n",
    "it represents. More broadly, recognizing numbers in photographs is a problem of interest to the optical\n",
    "character recognition community. While OCR on constrained domains like document processing is well\n",
    "studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises\n",
    "due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts,\n",
    "colours, styles, orientations, and character arrangements. The recognition problem is further complicated by\n",
    "environmental factors such as lighting, shadows, specularities, and occlusions as well as by image acquisition\n",
    "factors such as resolution, motion, and focus blurs. In this project, we will use the dataset with images centred\n",
    "around a single digit (many of the images do contain some distractors at the sides). Although we are taking a\n",
    "sample of the data which is simpler, it is more complex than MNIST because of the distractors\n",
    "\n",
    "## Dataset\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with the\n",
    "minimal requirement on data formatting but comes from a significantly harder, unsolved, real-world problem\n",
    "(recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google\n",
    "Street View images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing data and understanding shape of train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "#Opening the file as read only\n",
    "h5f = h5py.File('C:\\\\Users\\\\Srikanta\\\\Desktop\\\\Great Learning\\\\Python Files\\\\Week 30 Neural Networks Project\\\\SVHN_single_grey1.h5','r')\n",
    "\n",
    "h5f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading training, testing and validation datasets\n",
    "\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "X_val = h5f['X_val'][:]\n",
    "y_val = h5f['y_val'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dependent variable is (42000, 1024)\n",
      "Shape of the independent variable is (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Training data set\n",
    "print('Shape of the dependent variable is', X_train.shape)\n",
    "print('Shape of the independent variable is', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dependent variable is (18000, 1024)\n",
      "Shape of the independent variable is (18000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Testing data set\n",
    "print('Shape of the dependent variable is', X_test.shape)\n",
    "print('Shape of the independent variable is', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dependent variable is (60000, 1024)\n",
      "Shape of the independent variable is (60000,)\n"
     ]
    }
   ],
   "source": [
    "#Validation data set\n",
    "print('Shape of the dependent variable is', X_val.shape)\n",
    "print('Shape of the independent variable is', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1024,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-a96eb6b400e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Label: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2712\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2713\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2714\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m   2715\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2716\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m    705\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[1;32m--> 706\u001b[1;33m                             .format(self._A.shape))\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (1024,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMX0lEQVR4nO3bX4il9X3H8fenuxEak0aJk5DuKt2WNbotWnRiJPSPaWizay6WgBdqqFQCixBDLpVCk4I3zUUhBP8siyySm+xNJN0UEyktiQVr4yz4bxVlulKdrOAaQwoGKqvfXsxpc3q+szvPrGfO2cH3CwbmeZ7fOefLMOc9zzzzTKoKSRr3G/MeQNL5xzBIagyDpMYwSGoMg6TGMEhq1g1DksNJXk/y3BmOJ8m3kywneSbJNdMfU9IsDTljeAjYe5bj+4Ddo48DwAPvfSxJ87RuGKrqMeDNsyzZD3ynVj0BXJTkE9MaUNLsbZ/Cc+wAXh3bXhnte21yYZIDrJ5VcOGFF157xRVXTOHlJZ3JsWPH3qiqhY0+bhphyBr71rzPuqoOAYcAFhcXa2lpaQovL+lMkvznuTxuGn+VWAEuHdveCZycwvNKmpNphOEocNvorxPXA7+sqvZrhKStY91fJZJ8F7gBuCTJCvAN4AMAVXUQeAS4EVgGfgXcvlnDSpqNdcNQVbesc7yAr0xtIklz552PkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySmkFhSLI3yYtJlpPcvcbxjyT5QZKnkxxPcvv0R5U0K+uGIck24D5gH7AHuCXJnollXwGer6qrgRuAv09ywZRnlTQjQ84YrgOWq+pEVb0NHAH2T6wp4MNJAnwIeBM4PdVJJc3MkDDsAF4d214Z7Rt3L3AlcBJ4FvhaVb07+URJDiRZSrJ06tSpcxxZ0mYbEoassa8mtj8PPAX8NvCHwL1Jfqs9qOpQVS1W1eLCwsIGR5U0K0PCsAJcOra9k9Uzg3G3Aw/XqmXgZeCK6YwoadaGhOFJYHeSXaMLijcDRyfWvAJ8DiDJx4FPAiemOaik2dm+3oKqOp3kTuBRYBtwuKqOJ7ljdPwgcA/wUJJnWf3V466qemMT55a0idYNA0BVPQI8MrHv4NjnJ4G/mO5okubFOx8lNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVIzKAxJ9iZ5MclykrvPsOaGJE8lOZ7kJ9MdU9IsbV9vQZJtwH3AnwMrwJNJjlbV82NrLgLuB/ZW1StJPrZJ80qagSFnDNcBy1V1oqreBo4A+yfW3Ao8XFWvAFTV69MdU9IsDQnDDuDVse2V0b5xlwMXJ/lxkmNJblvriZIcSLKUZOnUqVPnNrGkTTckDFljX01sbweuBb4AfB74mySXtwdVHaqqxapaXFhY2PCwkmZj3WsMrJ4hXDq2vRM4ucaaN6rqLeCtJI8BVwMvTWVKSTM15IzhSWB3kl1JLgBuBo5OrPkH4I+TbE/yQeDTwAvTHVXSrKx7xlBVp5PcCTwKbAMOV9XxJHeMjh+sqheS/Ah4BngXeLCqntvMwSVtnlRNXi6YjcXFxVpaWprLa0vvF0mOVdXiRh/nnY+SGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJKaQWFIsjfJi0mWk9x9lnWfSvJOkpumN6KkWVs3DEm2AfcB+4A9wC1J9pxh3TeBR6c9pKTZGnLGcB2wXFUnqupt4Aiwf411XwW+B7w+xfkkzcGQMOwAXh3bXhnt+z9JdgBfBA6e7YmSHEiylGTp1KlTG51V0owMCUPW2FcT298C7qqqd872RFV1qKoWq2pxYWFh4IiSZm37gDUrwKVj2zuBkxNrFoEjSQAuAW5Mcrqqvj+NISXN1pAwPAnsTrIL+BlwM3Dr+IKq2vW/nyd5CPhHoyBtXeuGoapOJ7mT1b82bAMOV9XxJHeMjp/1uoKkrWfIGQNV9QjwyMS+NYNQVX/13seSNE/e+SipMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkppBYUiyN8mLSZaT3L3G8S8leWb08XiSq6c/qqRZWTcMSbYB9wH7gD3ALUn2TCx7GfjTqroKuAc4NO1BJc3OkDOG64DlqjpRVW8DR4D94wuq6vGq+sVo8wlg53THlDRLQ8KwA3h1bHtltO9Mvgz8cK0DSQ4kWUqydOrUqeFTSpqpIWHIGvtqzYXJZ1kNw11rHa+qQ1W1WFWLCwsLw6eUNFPbB6xZAS4d294JnJxclOQq4EFgX1X9fDrjSZqHIWcMTwK7k+xKcgFwM3B0fEGSy4CHgb+sqpemP6akWVr3jKGqTie5E3gU2AYcrqrjSe4YHT8IfB34KHB/EoDTVbW4eWNL2kypWvNywaZbXFyspaWluby29H6R5Ni5/JD2zkdJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBknNoDAk2ZvkxSTLSe5e43iSfHt0/Jkk10x/VEmzsm4YkmwD7gP2AXuAW5LsmVi2D9g9+jgAPDDlOSXN0JAzhuuA5ao6UVVvA0eA/RNr9gPfqVVPABcl+cSUZ5U0I9sHrNkBvDq2vQJ8esCaHcBr44uSHGD1jALgv5M8t6Fp5+sS4I15DzHQVpoVtta8W2lWgE+ey4OGhCFr7KtzWENVHQIOASRZqqrFAa9/XthK826lWWFrzbuVZoXVec/lcUN+lVgBLh3b3gmcPIc1kraIIWF4EtidZFeSC4CbgaMTa44Ct43+OnE98Muqem3yiSRtDev+KlFVp5PcCTwKbAMOV9XxJHeMjh8EHgFuBJaBXwG3D3jtQ+c89XxspXm30qywtebdSrPCOc6bqnYpQNL7nHc+SmoMg6Rm08OwlW6nHjDrl0YzPpPk8SRXz2POsXnOOu/Yuk8leSfJTbOcb2KGdWdNckOSp5IcT/KTWc84Mct63wsfSfKDJE+P5h1yXW1TJDmc5PUz3Rd0Tu+xqtq0D1YvVv4H8LvABcDTwJ6JNTcCP2T1XojrgX/fzJne46yfAS4efb5vXrMOnXds3b+weoH4pvN1VuAi4HngstH2x87nry3w18A3R58vAG8CF8xp3j8BrgGeO8PxDb/HNvuMYSvdTr3urFX1eFX9YrT5BKv3a8zLkK8twFeB7wGvz3K4CUNmvRV4uKpeAaiq833eAj6cJMCHWA3D6dmOORqk6rHR65/Jht9jmx2GM90qvdE1s7DROb7MaoXnZd15k+wAvggcnOFcaxnytb0cuDjJj5McS3LbzKbrhsx7L3AlqzfyPQt8rarenc14G7bh99iQW6Lfi6ndTj0Dg+dI8llWw/BHmzrR2Q2Z91vAXVX1zuoPtrkZMut24Frgc8BvAv+W5Imqemmzh1vDkHk/DzwF/Bnwe8A/JfnXqvqvTZ7tXGz4PbbZYdhKt1MPmiPJVcCDwL6q+vmMZlvLkHkXgSOjKFwC3JjkdFV9fyYT/trQ74M3quot4K0kjwFXA/MIw5B5bwf+rlZ/iV9O8jJwBfDT2Yy4IRt/j23yRZHtwAlgF7++iPP7E2u+wP+/MPLTOV3AGTLrZaze3fmZecy40Xkn1j/E/C4+DvnaXgn882jtB4HngD84j+d9APjb0ecfB34GXDLH74ff4cwXHzf8HtvUM4bavNup5zXr14GPAvePfgqfrjn9p93Aec8LQ2atqheS/Ah4BngXeLCq5vJv+QO/tvcADyV5ltU33F1VNZd/x07yXeAG4JIkK8A3gA+Mzbrh95i3REtqvPNRUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUvM/YA1djYGMYyEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show first number in the training dataset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "print('Label: ', X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first number in the testing dataset\n",
    "plt.imshow(X_test[0])\n",
    "plt.show()\n",
    "print('Label: ', y_test[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Reshaping the data and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped training dataset is (42000, 1024)\n",
      "Reshaped test dataset is (18000, 1024)\n",
      "Reshaped validation dataset is (60000, 1024)\n"
     ]
    }
   ],
   "source": [
    "#Reshaping Images into a single row/vector for a fully connected layer\n",
    "\n",
    "X_train = X_train.reshape(42000,1024)\n",
    "print('Reshaped training dataset is',X_train.shape)\n",
    "\n",
    "X_test = X_test.reshape(18000,1024)\n",
    "print('Reshaped test dataset is',X_test.shape)\n",
    "\n",
    "\n",
    "X_val = X_val.reshape(60000,1024)\n",
    "print('Reshaped validation dataset is',X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Normalizing data from 0 to 254.9745 to 0 to 1\n",
    "\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n",
    "X_val = X_val/X_val.max()\n",
    "\n",
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "print(X_test.max())\n",
    "print(X_test.min())\n",
    "print(X_val.max())\n",
    "print(X_val.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. One hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Creating a one hot vector corresponding to the labels of train and test data\n",
    "print(y_train[5])\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train,num_classes=10)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test,num_classes=10)\n",
    "\n",
    "print(y_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice above that the labels are converted to one hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Container module\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Fully connected layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Regularizer to for L2 regularization\n",
    "from keras.layers import Activation \n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "#Optimizer to train\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 256\n",
    "output_nodes = 10\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, )))\n",
    "model.add(BatchNormalization())                  \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "adam = optimizers.adam(learning_rate = 0.001)\n",
    "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 61s 1ms/step - loss: 2.3247 - accuracy: 0.0976\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 61s 1ms/step - loss: 2.3047 - accuracy: 0.1004\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 63s 1ms/step - loss: 2.3033 - accuracy: 0.0982\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 2.3032 - accuracy: 0.0989\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 58s 1ms/step - loss: 2.3032 - accuracy: 0.0975\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 59s 1ms/step - loss: 2.3031 - accuracy: 0.0989\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 57s 1ms/step - loss: 2.3033 - accuracy: 0.0992\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 56s 1ms/step - loss: 2.3031 - accuracy: 0.1012\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 59s 1ms/step - loss: 2.3031 - accuracy: 0.0998\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 62s 1ms/step - loss: 2.3033 - accuracy: 0.1009\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 2, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy seem to be in the correct range as there are 10 classes and the model is correctly predicting 1/10 = 0.1% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 47s 1ms/step - loss: 1.7478 - accuracy: 0.3845 0s - loss: 1.7523 - accura\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 23s 546us/step - loss: 1.2490 - accuracy: 0.5963\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 23s 542us/step - loss: 1.0585 - accuracy: 0.6629\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 23s 547us/step - loss: 0.9646 - accuracy: 0.6966s - loss: 0.9667 - accuracy: 0. - ETA: \n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 23s 538us/step - loss: 0.8954 - accuracy: 0.7147\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 23s 542us/step - loss: 0.8411 - accuracy: 0.7340\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 23s 541us/step - loss: 0.7909 - accuracy: 0.7526\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 23s 541us/step - loss: 0.7434 - accuracy: 0.7669\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 23s 555us/step - loss: 0.7091 - accuracy: 0.7790\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 23s 548us/step - loss: 0.6735 - accuracy: 0.7883\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 23s 549us/step - loss: 0.6511 - accuracy: 0.7927\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 23s 539us/step - loss: 0.6209 - accuracy: 0.8019s - loss:\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 23s 544us/step - loss: 0.5988 - accuracy: 0.8108\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 23s 541us/step - loss: 0.5819 - accuracy: 0.8166\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 24s 579us/step - loss: 0.5617 - accuracy: 0.8214\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 26s 618us/step - loss: 0.5496 - accuracy: 0.8274\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 24s 563us/step - loss: 0.5419 - accuracy: 0.8296s - loss: 0.5412 - \n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5225 - accuracy: 0.8345\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 21s 504us/step - loss: 0.5121 - accuracy: 0.8376\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 24s 570us/step - loss: 0.4987 - accuracy: 0.8415\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 0.4879 - accuracy: 0.8436\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 25s 597us/step - loss: 0.4784 - accuracy: 0.8475\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 0.4594 - accuracy: 0.8513\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 23s 550us/step - loss: 0.4578 - accuracy: 0.8517\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 24s 571us/step - loss: 0.4475 - accuracy: 0.8568\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 24s 561us/step - loss: 0.4324 - accuracy: 0.8600s - loss: 0.4328 - accuracy\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 23s 543us/step - loss: 0.4265 - accuracy: 0.8627\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 23s 546us/step - loss: 0.4218 - accuracy: 0.8652\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 0.4151 - accuracy: 0.8640\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 25s 591us/step - loss: 0.4025 - accuracy: 0.8684\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 25s 584us/step - loss: 0.3962 - accuracy: 0.8712\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 25s 597us/step - loss: 0.3884 - accuracy: 0.8739\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 0.3821 - accuracy: 0.8767\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 26s 626us/step - loss: 0.3823 - accuracy: 0.8756\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 0.3738 - accuracy: 0.8783s - loss: 0\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 24s 561us/step - loss: 0.3676 - accuracy: 0.8800\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 24s 573us/step - loss: 0.3595 - accuracy: 0.8825s - loss:\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 24s 572us/step - loss: 0.3534 - accuracy: 0.8845\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 24s 579us/step - loss: 0.3450 - accuracy: 0.8877\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 25s 587us/step - loss: 0.3458 - accuracy: 0.8873\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 27s 634us/step - loss: 0.3404 - accuracy: 0.8895\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 24s 575us/step - loss: 0.3318 - accuracy: 0.8919\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 24s 573us/step - loss: 0.3269 - accuracy: 0.8923\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 24s 569us/step - loss: 0.3167 - accuracy: 0.8965s - loss: 0.3161 - ac\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 27s 633us/step - loss: 0.3198 - accuracy: 0.8960\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 24s 560us/step - loss: 0.3184 - accuracy: 0.8965\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 25s 589us/step - loss: 0.3104 - accuracy: 0.8975\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 0.3051 - accuracy: 0.8990\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 24s 560us/step - loss: 0.3074 - accuracy: 0.9000\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 24s 569us/step - loss: 0.2997 - accuracy: 0.9016\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 24s 571us/step - loss: 0.2940 - accuracy: 0.9026\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 0.2930 - accuracy: 0.9038\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 0.2894 - accuracy: 0.9042\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 26s 615us/step - loss: 0.2821 - accuracy: 0.9055\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 24s 567us/step - loss: 0.2824 - accuracy: 0.9064\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 24s 565us/step - loss: 0.2798 - accuracy: 0.9091s - loss: 0.2791 -  - ETA: 0s - loss: 0.2800 - accuracy: 0.\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 24s 580us/step - loss: 0.2821 - accuracy: 0.9073\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 0.2746 - accuracy: 0.9090\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 0.2716 - accuracy: 0.9110\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 0.2714 - accuracy: 0.9111\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 24s 566us/step - loss: 0.2657 - accuracy: 0.9124\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 25s 585us/step - loss: 0.2647 - accuracy: 0.9116\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 24s 579us/step - loss: 0.2579 - accuracy: 0.9135\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 25s 586us/step - loss: 0.2602 - accuracy: 0.9137\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 25s 589us/step - loss: 0.2539 - accuracy: 0.9157\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 24s 562us/step - loss: 0.2512 - accuracy: 0.9177\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 24s 569us/step - loss: 0.2516 - accuracy: 0.9167\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 24s 565us/step - loss: 0.2461 - accuracy: 0.9185\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 24s 566us/step - loss: 0.2417 - accuracy: 0.9188\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 24s 562us/step - loss: 0.2488 - accuracy: 0.9167\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 25s 602us/step - loss: 0.2368 - accuracy: 0.9209\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 24s 564us/step - loss: 0.2452 - accuracy: 0.9173\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 25s 585us/step - loss: 0.2366 - accuracy: 0.9207\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 24s 574us/step - loss: 0.2393 - accuracy: 0.9203\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 0.2322 - accuracy: 0.9225\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 24s 568us/step - loss: 0.2275 - accuracy: 0.9255\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 24s 569us/step - loss: 0.2347 - accuracy: 0.9212\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 25s 597us/step - loss: 0.2262 - accuracy: 0.9240\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 24s 566us/step - loss: 0.2286 - accuracy: 0.9232\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 25s 588us/step - loss: 0.2182 - accuracy: 0.9271\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 25s 585us/step - loss: 0.2257 - accuracy: 0.9254s - loss: 0.2255 - \n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 24s 567us/step - loss: 0.2248 - accuracy: 0.9251\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 25s 584us/step - loss: 0.2214 - accuracy: 0.9262\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 23s 558us/step - loss: 0.2144 - accuracy: 0.9287s - loss: 0.213\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 25s 605us/step - loss: 0.2210 - accuracy: 0.9267\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 24s 573us/step - loss: 0.2157 - accuracy: 0.9281\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 24s 575us/step - loss: 0.2079 - accuracy: 0.9310s - loss: 0\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 26s 614us/step - loss: 0.2137 - accuracy: 0.9284\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 0.2062 - accuracy: 0.9309\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 24s 568us/step - loss: 0.2105 - accuracy: 0.9284\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 25s 584us/step - loss: 0.2048 - accuracy: 0.9320\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 24s 564us/step - loss: 0.2017 - accuracy: 0.9323\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 0.2016 - accuracy: 0.9325\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 24s 578us/step - loss: 0.1974 - accuracy: 0.9337\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 24s 561us/step - loss: 0.2087 - accuracy: 0.9298\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 0.2035 - accuracy: 0.9326\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 24s 563us/step - loss: 0.2007 - accuracy: 0.9323\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 0.1948 - accuracy: 0.9351\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 24s 561us/step - loss: 0.1885 - accuracy: 0.9370s - loss: 0.1886 - ac\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 24s 570us/step - loss: 0.1975 - accuracy: 0.9340\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 2s 122us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.6934842919376162\n",
      "Test accuracy:  0.8260555267333984\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: ', results[0])\n",
    "print('Test accuracy: ', results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
